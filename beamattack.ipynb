{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e017ad263351b5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "This jupyter notebook should always contain the latest implementation of our attack.\n",
    "If you want to use it, duplicate it. Please also update it if you have an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a8209e38033d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:41:04.197262Z",
     "start_time": "2024-05-03T17:40:59.225489Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, AutoModelForMaskedLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a02f09ece3f5bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809dc113a847aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:41:05.105243Z",
     "start_time": "2024-05-03T17:41:04.198469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if started in repository\n",
    "if Path.cwd().absolute().match(\"ml4nlp2-adversarial-attack\"): \n",
    "    REPO_PATH = Path.cwd().absolute()\n",
    "else:\n",
    "    # Specify the path to the repository\n",
    "    BASE_PATH= Path(\"/kaggle/working/\")\n",
    "    REPO_PATH = BASE_PATH / \"ml4nlp2-adversarial-attack\"\n",
    "    assert BASE_PATH.exists(), \"Base path not found. Please change, where you want to have the repo installed.\"\n",
    "\n",
    "DATA_BASE_PATH = REPO_PATH / \"clef2024-checkthat-lab\" / \"task6\" /\"incrediblAE_public_release\"\n",
    "OUTPUT_DIR = REPO_PATH / \"output\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # used for the victim models\n",
    "print(f\"DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712012caa40dbc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Create Repository: (In case this notebook is uploaded without the repository and files:)\n",
    "Note: Skip this section if the repository already exists on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94650305a80b53e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T19:23:05.448209Z",
     "start_time": "2024-04-10T19:23:05.425122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if REPO_PATH.exists():\n",
    "    os.chdir(REPO_PATH)\n",
    "    raise Exception(\"Repository already exists.\")\n",
    "\n",
    "# Cloning Repostiory\n",
    "!git config --global user.email \"<User Email>\"\n",
    "!git config --global user.name \"<User Name>\"\n",
    "!git clone --recurse-submodules https://<Github Token>@github.com/<User Name>/ml4nlp2-adversarial-attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490b0c5dc0fd2ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Special Imports\n",
    "(Imports that need be installed manually, and are not default in CLoud Computing Environments (Colab, Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cfb343255872f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(REPO_PATH)\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7a6fdca87c9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:41:05.605004Z",
     "start_time": "2024-05-03T17:41:05.106464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(REPO_PATH, \"BODEGA\"))\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "from BODEGA.victims.bert import VictimBERT\n",
    "from BODEGA.victims.bilstm import VictimBiLSTM\n",
    "from BODEGA.victims.caching import VictimCache\n",
    "from bleurt_pytorch import BleurtForSequenceClassification, BleurtTokenizer\n",
    "\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import OpenAttack\n",
    "from OpenAttack import Victim\n",
    "from OpenAttack.attack_assist.goal import ClassifierGoal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23515489a68dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:41:06.183818Z",
     "start_time": "2024-05-03T17:41:05.946996Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from beam_attack.infrastructure_helper_functions import get_incredible_dataset, free_up_model_space\n",
    "import beam_attack.attack as beam_attack\n",
    "from beam_attack import BodegaAttackEvaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1001a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:41:06.488271Z",
     "start_time": "2024-05-03T17:41:06.477164Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.data_mappings import SEPARATOR\n",
    "import pathlib\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 512\n",
    "EPOCHS = 5\n",
    "MAX_BATCHES = -1\n",
    "pretrained_model = \"roberta-base\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_loop(model, eval_dataloader, device, skip_visual=False):\n",
    "    print(\"Evaluating...\")\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(range(len(eval_dataloader)), ascii=True, disable=skip_visual)\n",
    "    correct = 0\n",
    "    size = 0\n",
    "    TPs = 0\n",
    "    FPs = 0\n",
    "    FNs = 0\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        # print(logits)\n",
    "        # a = input()\n",
    "        pred = torch.argmax(logits, dim=-1).detach().to(torch.device('cpu')).numpy()\n",
    "        Y = batch[\"labels\"].to(torch.device('cpu')).numpy()\n",
    "        eq = numpy.equal(Y, pred)\n",
    "        size += len(eq)\n",
    "        correct += sum(eq)\n",
    "        TPs += sum(numpy.logical_and(numpy.equal(Y, 1.0), numpy.equal(pred, 1.0)))\n",
    "        FPs += sum(numpy.logical_and(numpy.equal(Y, 0.0), numpy.equal(pred, 1.0)))\n",
    "        FNs += sum(numpy.logical_and(numpy.equal(Y, 1.0), numpy.equal(pred, 0.0)))\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # print(Y)\n",
    "        # print(pred)\n",
    "        # a = input()\n",
    "\n",
    "        if i == MAX_BATCHES:\n",
    "            break\n",
    "    print('Accuracy: ' + str(correct / size))\n",
    "    print('F1: ' + str(2 * TPs / (2 * TPs + FPs + FNs)))\n",
    "    print(correct, size, TPs, FPs, FNs)\n",
    "\n",
    "    results = {\n",
    "        'Accuracy': correct/size,\n",
    "        'F1': 2 * TPs / (2 * TPs + FPs + FNs)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "class VictimRoBERTa(OpenAttack.Classifier):\n",
    "    def __init__(self, path, task, device=torch.device('cpu')):\n",
    "        self.device = device\n",
    "        config = AutoConfig.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.with_pairs = (task == 'FC' or task == 'C19')\n",
    "\n",
    "    def get_pred(self, input_):\n",
    "        return self.get_prob(input_).argmax(axis=1)\n",
    "\n",
    "    def get_prob(self, input_):\n",
    "        try:\n",
    "            probs = None\n",
    "            # print(len(input_), input_)\n",
    "\n",
    "            batched = [input_[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] for i in\n",
    "                       range((len(input_) + BATCH_SIZE - 1) // BATCH_SIZE)]\n",
    "            for batched_input in batched:\n",
    "                if not self.with_pairs:\n",
    "                    tokenised = self.tokenizer(batched_input, truncation=True, padding=True, max_length=MAX_LEN,\n",
    "                                               return_tensors=\"pt\")\n",
    "                else:\n",
    "                    parts = [x.split(SEPARATOR) for x in batched_input]\n",
    "                    tokenised = self.tokenizer([x[0] for x in parts], [(x[1] if len(x) == 2 else '') for x in parts],\n",
    "                                               truncation=True, padding=True,\n",
    "                                               max_length=MAX_LEN,\n",
    "                                               return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    tokenised = {k: v.to(self.device) for k, v in tokenised.items()}\n",
    "                    outputs = self.model(**tokenised)\n",
    "                probs_here = torch.nn.functional.softmax(outputs.logits, dim=-1).to(torch.device('cpu')).numpy()\n",
    "                if probs is not None:\n",
    "                    probs = numpy.concatenate((probs, probs_here))\n",
    "                else:\n",
    "                    probs = probs_here\n",
    "            return probs\n",
    "        except Exception as e:\n",
    "            # Used for debugging\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8437d44b4896f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CODE: Beam Attack Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba94e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamAttacker(OpenAttack.attackers.ClassificationAttacker):\n",
    "    # imp options: \"lime\", \"random\", \"bert\" (for the bert-style word importances)\n",
    "    def __init__(self, device, verbose=False, k=20, width=None, early_stop=1, temperature=1.0, \n",
    "                 imp=\"lime\", lime_num_features=5000, lime_num_samples=5000, lime_kernel_width=25,\n",
    "                 filter_beams_with_only_negative=False, only_positive_importances=False, remove_words=True, keep_original=True, \n",
    "                 positional=True, add_semantic_pruning=False, semantic_pruning_ratio=0.0):\n",
    "        self.imp = imp\n",
    "        self.lime_num_features = lime_num_features\n",
    "        self.lime_num_samples = lime_num_samples\n",
    "        self.temperature = temperature\n",
    "        self.verbose = verbose\n",
    "        self.k = k\n",
    "        self.positional = positional\n",
    "        if width:\n",
    "            self.width = width\n",
    "        else:\n",
    "            self.width = k\n",
    "        self.early_stop = early_stop\n",
    "        self.filter_beams_with_only_negative = filter_beams_with_only_negative\n",
    "        self.only_positive_importances = only_positive_importances\n",
    "        self.remove_words = remove_words\n",
    "        self.keep_original = keep_original\n",
    "        self.add_semantic_pruning = add_semantic_pruning\n",
    "        self.semantic_pruning_ratio = semantic_pruning_ratio\n",
    "        self.device = device\n",
    "        \n",
    "        # self.bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "        self.roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "        # self.pipe = pipeline(\"fill-mask\", model=\"roberta-large\", tokenizer=self.roberta_tokenizer, device=self.device)\n",
    "        self.roberta_model = AutoModelForMaskedLM.from_pretrained('roberta-large').to(device)\n",
    "        # self.roberta_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-large')\n",
    "        # self.roberta_model = AutoModelForMaskedLM.from_pretrained('vinai/bertweet-large').to(device)\n",
    "        \n",
    "        if self.imp == \"lime\":\n",
    "            self.explainer = LimeTextExplainer(verbose=verbose, kernel_width=lime_kernel_width, bow=not positional)\n",
    "        else:\n",
    "            self.explainer = None\n",
    "        \n",
    "        self.result_out = []\n",
    "\n",
    "        # if we have early_stop > 1, we get multiple candidates and calculate their bodega scores and return the best one.\n",
    "        if self.early_stop > 1 or self.add_semantic_pruning:\n",
    "            # Load BLEURT model and tokenizer\n",
    "            self.bleurt_model = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20')\n",
    "            self.bleurt_tokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20')\n",
    "            # self.bleurt_model = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\n",
    "            # self.bleurt_tokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n",
    "        else:\n",
    "            self.scorer = None\n",
    "            \n",
    "        print(\"Beam_Attack: Initialized: \", self.k, self.width, self.early_stop)\n",
    "\n",
    "    def attack(self, victim : VictimCache, input_ : str, goal : ClassifierGoal):      \n",
    "        beam_outputs, target_class, initial_proba = beam_attack.attack_text_bfs(text=input_, victim=victim, \n",
    "                device=self.device, roberta_model=self.roberta_model, roberta_tokenizer = self.roberta_tokenizer, k=self.k, width=self.width, \n",
    "                verbose=self.verbose, early_stop=self.early_stop,  imp=self.imp, temperature=self.temperature, explainer=self.explainer,\n",
    "                lime_num_features=self.lime_num_features, lime_num_samples=self.lime_num_samples,\n",
    "                filter_beams_with_only_negative=self.filter_beams_with_only_negative, only_positive_importances=self.only_positive_importances, \n",
    "                remove_words=self.remove_words, keep_original=self.keep_original, add_semantic_pruning=self.add_semantic_pruning, \n",
    "                semantic_pruning_ratio=self.semantic_pruning_ratio, bleurt_model=self.bleurt_model, bleurt_tokenizer=self.bleurt_tokenizer, positional=self.positional)\n",
    "        \n",
    "        self.result_out.append({\"output\":beam_outputs, \"target_class\": target_class, \"initial_probability\": initial_proba})\n",
    "        \n",
    "        if not any(beam_outputs):\n",
    "            return None\n",
    "        elif len(beam_outputs) > 1:\n",
    "            semantic_similarities = beam_attack.calculate_bleurt_score(self.bleurt_model, self.bleurt_tokenizer, \n",
    "                            [input_]*len(beam_outputs), [beam[0] for beam in beam_outputs], device=self.device)\n",
    "            return beam_outputs[np.argmax(semantic_similarities)][0]\n",
    "        \n",
    "        return beam_outputs[0][0]\n",
    "               \n",
    "    def save_results(self, path):\n",
    "        with open(path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"output\", \"target_class\", \"initial_probability\"])\n",
    "            for row in self.result_out:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(\"Beam_Attack: Results saved at: \", path.absolute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48bd7d37cf1002",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1052de0bb559e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:45:12.515965Z",
     "start_time": "2024-05-03T17:45:10.445788Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = \"RD\"\n",
    "first_n_samples = None\n",
    "\n",
    "task_path = DATA_BASE_PATH / task\n",
    "\n",
    "victim_model = 'surprise' # BERT or BiLSTM or surprise\n",
    "model_path = Path(os.path.join(task_path, f\"{victim_model}-512.pth\"))\n",
    "if victim_model == 'BERT':\n",
    "    victim = VictimCache(model_path, VictimBERT(model_path, task, DEVICE))\n",
    "    victim_model_generator =\"bert-style\"\n",
    "elif victim_model == 'BiLSTM':\n",
    "    victim = VictimCache(model_path, VictimBiLSTM(model_path, task, DEVICE))\n",
    "    victim_model_generator =\"bert-style\"\n",
    "elif victim_model == 'surprise':\n",
    "    victim = VictimCache(model_path, VictimRoBERTa(model_path, task, DEVICE))\n",
    "    victim_model_generator =\"surprise\"\n",
    "\n",
    "# Prepare victim\n",
    "print(\"Loading up victim model...\")\n",
    "dataset, with_pairs = get_incredible_dataset(task, task_path, victim_model_generator=victim_model_generator,subset=\"attack\", first_n_samples=first_n_samples, randomised=False)\n",
    "SEPARATOR_CHAR = '~'\n",
    "SEPARATOR = ' ' + SEPARATOR_CHAR + ' '\n",
    "\n",
    "dataset = dataset.select(indices=range(300, len(dataset)))\n",
    "\n",
    "print(\"Dataset: \", dataset.shape, dataset.features)\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=False, exist_ok=True)\n",
    "results_path = OUTPUT_DIR / (\"results_\" + task + \"_\" + f\"_{victim_model}.txt\")\n",
    "beam_attack_log_path = OUTPUT_DIR / (\"beam_attack_log_\" + task + f\"_{victim_model}\" + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701eb4229d530ceb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attacker = BeamAttacker(device=DEVICE, verbose=False, k=10, width=30, early_stop=10, imp=\"bert\", lime_num_samples=500, temperature=1)\n",
    "scorer = BodegaAttackEvaluations.setup_bodega_scorer_and_run_attacks(attacker, dataset, DEVICE, OUTPUT_DIR, victim, results_path=results_path, task=task, victim_name=victim_model)\n",
    "\n",
    "BodegaAttackEvaluations.calculate_metrics(scorer, results_path=results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "victim.finalise()\n",
    "attacker.save_results(beam_attack_log_path)\n",
    "del victim\n",
    "del attacker\n",
    "free_up_model_space()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
